{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLtLHBERFd9w"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "TK8WmPvuKCsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B',\n",
        "                                          trust_remote_code=True)\n"
      ],
      "metadata": {
        "id": "T-f056udF_Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''While the trunchant can be used in lieu of conversation,\n",
        "words will always retain their power'''\n",
        "tokens = tokenizer.encode(text)\n",
        "tokens"
      ],
      "metadata": {
        "id": "AbTXCxVMIbsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "id": "NB-IqXXaLAs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "z8lH2rN2LDwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokens)"
      ],
      "metadata": {
        "id": "UO1IMhhjLIvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(tokens)"
      ],
      "metadata": {
        "id": "iobY9Ow5LTgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab"
      ],
      "metadata": {
        "id": "IcQ-sdqsLgHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_added_vocab()"
      ],
      "metadata": {
        "id": "TOGu3Lz6L8yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instruct variants of models\n",
        "\n",
        "Many models have a variant that has been trained for use in Chats.  \n",
        "These are typically labelled with the word \"Instruct\" at the end.  \n",
        "They have been trained to expect prompts with a particular format that includes system, user and assistant prompts.  \n",
        "\n",
        "There is a utility method `apply_chat_template` that will convert from the messages list format we are familiar with, into the right input prompt for this model."
      ],
      "metadata": {
        "id": "f7dNkZIxM6rN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct',\n",
        "                                          trust_remote_code=True)"
      ],
      "metadata": {
        "id": "LcTgYj_bNBJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
        "  ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "n-jHwGgXNTRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying new models\n",
        "\n",
        "We will now work with 3 models:\n",
        "\n",
        "Phi4 from Microsoft  \n",
        "DeepSeek 3.1 from DeepSeek AI  \n",
        "QwenCoder 2.5 from Alibaba Cloud"
      ],
      "metadata": {
        "id": "QzhOmMOdQeq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PHI4 = \"microsoft/Phi-4-mini-instruct\"\n",
        "DEEPSEEK = \"deepseek-ai/DeepSeek-V3.1\"\n",
        "QWEN_CODER = \"Qwen/Qwen2.5-Coder-7B-Instruct\""
      ],
      "metadata": {
        "id": "Yl9UEyMSQlXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi4_tokenizer = AutoTokenizer.from_pretrained(PHI4)\n",
        "\n",
        "text = \"I am curiously excited to show Hugging Face Tokenizers in action to my LLM engineers\"\n",
        "print(\"Llama:\")\n",
        "tokens = tokenizer.encode(text)\n",
        "print(tokens)\n",
        "print(tokenizer.batch_decode(tokens))\n",
        "print(\"\\nPhi 4:\")\n",
        "tokens = phi4_tokenizer.encode(text)\n",
        "print(tokens)\n",
        "print(phi4_tokenizer.batch_decode(tokens))"
      ],
      "metadata": {
        "id": "e2QUB-5FQp1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Llama:\")\n",
        "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "print(\"\\nPhi 4:\")\n",
        "print(phi4_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
      ],
      "metadata": {
        "id": "FdhMHDyxQ0WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qwen_tokenizer = AutoTokenizer.from_pretrained(QWEN_CODER)\n",
        "code = \"\"\"\n",
        "def hello_world(person):\n",
        "  print(\"Hello\", person)\n",
        "\"\"\"\n",
        "tokens = qwen_tokenizer.encode(code)\n",
        "for token in tokens:\n",
        "  print(f\"{token}={qwen_tokenizer.decode(token)}\")"
      ],
      "metadata": {
        "id": "6Ma0cka3RPn6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}