{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes"
      ],
      "metadata": {
        "id": "B8Besitylxvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej7y7Kwuig5N"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import gradio as gr\n",
        "from transformers import pipeline, AutoTokenizer, TextStreamer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "inY1fP41tISl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential = True)"
      ],
      "metadata": {
        "id": "pwvtBRqfjrnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONSTANTS\n",
        "LLAMA = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "  You are an AI assistant who generates syntethic test data set for various different kinds of fileds. You should create synthetic yet believable datasets that can be valueable for\n",
        "  different kinds of businesses or research purpose. Just answer to the datasets request with near-real-world data in json format.\n",
        "  For example, let's say user asks for fake university students dataset for study habits, you might reply with:\n",
        "  {\n",
        "    \"dataset\": [\n",
        "      {\n",
        "        \"student-id\": \"1\",\n",
        "        \"courses\":...,\n",
        "        \"hours_studied\":...,\n",
        "        \"hours_in_sports\":...,\n",
        "        \"grades\":...\n",
        "      },\n",
        "      .\n",
        "      .\n",
        "      .\n",
        "    ]\n",
        "  }\n",
        "Improvise to make things better if you see fit.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "6gB5Q1gfkKvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model\n",
        "\n",
        "def generate(model, data_generation_topic):\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": system_prompt},\n",
        "      {\"role\": \"user\", \"content\": data_generation_topic}\n",
        "  ]\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors='pt').to('cuda')\n",
        "  streamer = TextStreamer(tokenizer)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model, device_map='auto',\n",
        "                                               quantization_config=quant_config)\n",
        "  outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)\n",
        "  del tokenizer, streamer, model, inputs, outputs\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "4vZnsReGluqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(LLAMA, \"Generate data on spending habits of 5 random people from Nepal\")"
      ],
      "metadata": {
        "id": "FqIEOa8DmJq-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}