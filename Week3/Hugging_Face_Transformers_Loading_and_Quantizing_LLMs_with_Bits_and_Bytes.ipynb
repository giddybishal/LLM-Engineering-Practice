{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "XtWzaIksWtrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1zqL_0uTgCk"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "U-3Qt400UknU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LLAMA = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "PHI = \"microsoft/Phi-4-mini-instruct\"\n",
        "GEMMA = \"google/gemma-3-270m-it\"\n",
        "QWEN = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "DEEPSEEK = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\""
      ],
      "metadata": {
        "id": "qvpcqb9IU-gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Tell a joke for a room of Data Scientists\"}\n",
        "  ]"
      ],
      "metadata": {
        "id": "51FQWwuAVSKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization Config - this allows us to load the model into memory and use less memory\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "nVLUmRn9VU4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "# inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "I1XWnaAZWPHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs"
      ],
      "metadata": {
        "id": "2Cb52gpqX5Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The model\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)"
      ],
      "metadata": {
        "id": "vBEkU1d9YAz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "yhaeOo8OYnk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# memory = model.get_memory_footprint()/1e6\n",
        "# print(f'Memory footprint:{memory:,.1f}MB')"
      ],
      "metadata": {
        "id": "Qeamb_79qVAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs = model.generate(inputs, max_new_tokens = 80)\n",
        "# print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "vqNhISYxr5Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete Python references\n",
        "# del inputs, outputs, model, tokenizer\n",
        "\n",
        "# Clear GPU cache\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# Optional: trigger garbage collection to free CPU memory too\n",
        "# import gc\n",
        "# gc.collect()"
      ],
      "metadata": {
        "id": "4W1XMXu_tuCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapping everything in a function - and  adding Streaming\n",
        "\n",
        "def generate(model, messages):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors='pt').to('cuda')\n",
        "  streamer = TextStreamer(tokenizer)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model, device_map='auto',\n",
        "                                               quantization_config=quant_config)\n",
        "  outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)\n",
        "  del tokenizer, streamer, model, inputs, outputs\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "udutYR_Yujoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yHlWON_oxkEf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}