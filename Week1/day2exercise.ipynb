{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "5069ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import ollama\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "18a08a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = 'http://localhost:11434/api/chat'\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = 'llama3.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d40a66de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [{\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "bf6c6695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api call behind the scene that ollama.chat does:\n",
    "\n",
    "# payload = {\n",
    "#     'model': MODEL,\n",
    "#     \"messages\": messages,\n",
    "#     \"stream\": False\n",
    "# }\n",
    "\n",
    "# response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "# print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4def5f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = ollama.chat(model=MODEL, messages=messages, stream=True)\n",
    "# for chunk in response:\n",
    "#     print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9ae4d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish.\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a witty assistant who writes short, humorous, and light-hearted summaries\n",
    "of websites. Be playful but never rude or disrespectful.\n",
    "Avoid sarcasm that targets people personally.\n",
    "Respond in markdown (no code block).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8f251722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our user prompt\n",
    "\n",
    "user_prompt_prefix = \"\"\"\n",
    "Here are the contents of a website.\n",
    "Provide a short summary of this website.\n",
    "If it includes news or announcements, then summarize these too.\n",
    "Avoid bullet points and include at least 200 words.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "933dae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://edwarddonner.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "cbc70d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaraping data from a website\n",
    "\n",
    "def fetch_website_contents(url):\n",
    "\n",
    "    # Step 1: Fetch the page content\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "\n",
    "    # Step 2: Parse it with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Step 3: Extract information\n",
    "    # Example: get all <p> (paragraph) text\n",
    "    paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
    "\n",
    "    # Step 4: Combine text\n",
    "    text = \"\\n\".join(paragraphs)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "71e02a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_summarize_text = fetch_website_contents(url)\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_prefix + website}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "cbfd7e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Meet Ed Donner: The Tech Enthusiast Behind Nebula.io**\n",
       "\n",
       "Welcome to the website of Ed Donner, a co-founder and CTO of Nebula.io, where AI is being used to revolutionize talent discovery. Ed's passion for coding, artificial intelligence, and electronic music production (he's still working on getting back into DJing) shines through in this engaging website.\n",
       "\n",
       "Nebula.io aims to utilize the power of Large Language Models (LLMs) to help individuals find their purpose and pursue their passions. The platform has already gained traction among recruiters, who use it to source, understand, engage, and manage top talent. Ed's previous venture, untapt, was acquired in 2021, a testament to his innovative spirit.\n",
       "\n",
       "The website showcases Nebula.io's proprietary LLMs, which have been verticalized for the talent industry. The company has even patented its matching model! With an award-winning platform and glowing press coverage, it's no wonder that customers are singing its praises.\n",
       "\n",
       "If you're as fascinated by AI and tech as Ed is, be sure to connect with him through his email address (ed [at] edwarddonner [dot] com). Who knows? You might just discover a new passion or collaboration opportunity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat(model=MODEL, messages=messages_for(to_summarize_text), stream=True)\n",
    "\n",
    "# create a display placeholder\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "full_text = \"\"\n",
    "for chunk in response:\n",
    "    full_text += chunk['message']['content']\n",
    "    # update the same output cell in real-time\n",
    "    display_handle.update(Markdown(full_text))\n",
    "    time.sleep(0.05)  # optional small delay to smooth rendering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
