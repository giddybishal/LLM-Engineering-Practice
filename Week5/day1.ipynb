{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c9cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# from huggingface_hub import InferenceClient\n",
    "import ollama\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import gradio as gr\n",
    "# from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0707729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "# hf_key = os.getenv('HF_KEY')\n",
    "# MODEL = 'deepseek-ai/DeepSeek-V3.1'\n",
    "# client = InferenceClient(\n",
    "#     provider='novita',\n",
    "#     api_key=hf_key\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be6cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = {}\n",
    "\n",
    "employees = glob.glob(\"knowledge-base/employees/*\")\n",
    "\n",
    "for employee in employees:\n",
    "    # More robust way to extract name from file path\n",
    "    name = os.path.splitext(os.path.basename(employee))[0].split(' ')[-1]\n",
    "    try:\n",
    "        with open(employee, 'r', encoding='utf-8') as f:\n",
    "            doc = f.read()\n",
    "        context[name] = doc\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {employee}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fe4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = glob.glob(\"knowledge-base/products/*\")\n",
    "\n",
    "for product in products:\n",
    "    # More robust way to extract name from file path\n",
    "    name = os.path.splitext(os.path.basename(product))[0].split(' ')[-1]\n",
    "    try:\n",
    "        with open(product, 'r', encoding='utf-8') as f:\n",
    "            doc = f.read()\n",
    "        context[name] = doc\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {product}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6dfe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an expert in answering accurate questions about Insurellm, the Insurance Tech company. Give brief, accurate answers. If you don't know the answer, say so. Do not make anything up if you haven't been provided with relevant context.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ca1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_context(message):\n",
    "    relevant_context = []\n",
    "    for context_title, context_details in context.items():\n",
    "        if context_title.lower() in message.lower():\n",
    "            relevant_context.append(context_details)\n",
    "\n",
    "    return relevant_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad876932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context(message):\n",
    "    relevant_context = get_relevant_context(message)\n",
    "    if relevant_context:\n",
    "        message += \"\\n\\nThe following additional context might be relevant in answering this question:\\n\\n\"\n",
    "        for relevant in relevant_context:\n",
    "            message += relevant + \"\\n\\n\"\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use local ollama\n",
    "ollama_model = 'gpt-oss:120b-cloud'\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{'role': 'system', 'content': system_message}]\n",
    "    for user_message, assisstant_message in history:\n",
    "        messages.append({'role': 'user', 'content':user_message})\n",
    "        messages.append({'role':'system', 'content': assisstant_message})\n",
    "\n",
    "    message = add_context(message)\n",
    "    messages.append({'role':'user', 'content': message})\n",
    "\n",
    "    response = ollama.chat(model=ollama_model, messages=messages, stream=True)\n",
    "    result = ''\n",
    "    for chunk in response:\n",
    "        result += chunk['message']['content']\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fea778",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn = chat).launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
