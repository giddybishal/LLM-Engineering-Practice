{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0338aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0794542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# ** HuggingFaceEmbeddings requires sentence-transformers package\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d264b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader,\n",
    "    loader_kwargs=text_loader_kwargs, use_multithreading=True)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ae2e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1546b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ec048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a Chroma Datastore already exists - if so, delete the collection to start from scratch\n",
    "\n",
    "db_name = 'vector_db'\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472dacf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our Chroma vectorstore!\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f'Vectorstore created with {vectorstore._collection.count()} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9b7e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a new Chat with OpenAI\n",
    "# llm = ChatOllama(temperature=0.7, model='gpt-oss:120b-cloud')\n",
    "\n",
    "# # set up the conversation memory for the chat\n",
    "# memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# # the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "# retriever = vectorstore.as_retriever()\n",
    "\n",
    "# # putting it together: set up the conversation chain with the LLM, the vector store and memory\n",
    "# conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea4a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping that in a function\n",
    "\n",
    "# def chat(message, history):\n",
    "#     result = conversation_chain.invoke({\"question\": message})\n",
    "#     return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947298da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e8c0f2",
   "metadata": {},
   "source": [
    "# The LLM cant give the name of the winner of the 2023 IIOTY award winner. Let's investigate waht gets sent behind the scenes to see how we may solve this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec9c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "\n",
    "llm = ChatOllama(temperature=0.7, model='gpt-oss:120b-cloud')\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "# k is how many chnks to use when using retriever, we will take more chunks to provide better contexts for the LLM answer\n",
    "retriever = vectorstore.as_retriever(search_kwargs = {\"k\":25})\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, callbacks=[StdOutCallbackHandler()])\n",
    "\n",
    "# query = \"Who received the IIOTY award in 2023?\"\n",
    "# result = conversation_chain.invoke({\"question\":query})\n",
    "# answer = result[\"answer\"]\n",
    "# print(\"\\nAnswer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eced6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3406355",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
